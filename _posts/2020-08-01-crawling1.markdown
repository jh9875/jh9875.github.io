---
layout: post
title:  "크롤링 연습해보기"
date:   2020-08-01
categories: PLAY
permalink: /archivers/crawling1
tags: [python, crawling, BeautifulSoup]
---

## 크롤링이란?

이번 포스팅에서는 파이썬과 BeautifulSoup를 이용해서 크롤링을 간단하게 다뤄보겠습니다.   

*크롤링*이란 웹 페이지를 그대로 가져와서 데이터를 추출해 내는 행위다. 라고 나무위키에 써있습니다.   

웹에서 특정 정보를 가져오고 이것을 이용하고 싶을때 크롤링 하는 방법을 알면 유용할 것 같습니다.   

## 기본 설정

크롤링을 처음 접해볼 때 가장 많이 사용하는 것이 파이썬의 BeautifulSoup입니다.   

BeautifulSoup는 HTML 및 XML문서를 구문 분석하기 위한 파이썬의 라이브러리인데 사용하기 간단해서 
많이 이용되는 것 같습니다.   

BeautifulSoup를 설치하는 방법도 아주 간단합니다. 다음 명령어를 명령 프롬프트에 입력하면 됩니다.   

> pip install bs4

아주 간단하게 설치가 끝났습니다.   

또 설치 해줘야 하는 것이 있는데 파이썬의 requests입니다.   

requests는 파이썬에서 http 요청을 간결하게 사용하도록 도와주는 라이브러리 입니다.   

이것 또한 pip 명령어로 설치 가능합니다.   

> pip install requests

## 페이지 정보 얻기

위에서 bs4와 requests를 설치했으면 이제 시작하면 됩니다.   

일단 가장먼저 import를 해줘야겠죠?   

~~~python
import requests
from bs4 import BeautifulSoup
~~~

이제 정보를 얻고자 하는 사이트의 주소와 requests를 이용해서 페이지의 html코드를 읽어옵니다.   

~~~python
import requests
from bs4 import BeautifulSoup

url ='https://movie.naver.com/movie/sdb/rank/rmovie.nhn'
html_text =requests.get(url).text;
~~~ 

위에서 requests.get(url) 메서드는 웹 서비스를 요청하는 메서드입니다.   

print( requests(get(url)) )을 출력해보면 "<Response [200]>"가 출력이 됩니다.   

여기서 응답 200은 요청이 성공적으로 되었다는 의미로 쉽게말해 ok를 의미합니다.   

그럼 url에서 웹 서비스를 이용이 가능합니다.   

requests.get(url).text를 하면 페이지의 html코드가 저장이 됩니다.   

위 코드에서 print(html_text)를 출력해보면 해당 페이지에서 마우스 우클릭 -> 페이지 소스 보기를 누르면 나오는 코드랑 똑같은 내용이 들어 있습니다.   

자 이제 BeautifulSoup를 이용해 보겠습니다.   

~~~python
import requests
from bs4 import BeautifulSoup

url ='https://movie.naver.com/movie/sdb/rank/rmovie.nhn'
html_text =requests.get(url).text;
soup_obj =BeautifulSoup(html_text, 'html.parser')
~~~

위 코드에서 마지막줄에 soup_obj에 BeautifulSoup객체를 저장해 주었습니다.   

BeautifulSoup는 두개의 인자를 받는데 첫번째 받는 인자는 html이나 xml로 작성된 문자열 이어야 합니다.   

두번째 인자는 첫번째 인자를 어떻게 해석할 것인지에 대한 것인데, xml등 다른 것들도 있지만 여기서는 html로 해석할 것이므로 html.parser를 이용하겠습니다.   

위 코드까지 마치고 soup_obj를 print로 출력해보면 html코드가 나옵니다.   

다만 위에서 html_text와 차이점은 공백이 많이 줄어 들었고 코드가 정리되어있다는 느낌이 듭니다.   

그리고 BeautifulSoup가 제공하는 메서드를 이용할 수 있습니다.   

그 다음으로 html코드에서 특정 정보를 가져오는 코드를 작성해 보겠습니다.   

~~~python
import requests
from bs4 import BeautifulSoup

url ='https://movie.naver.com/movie/sdb/rank/rmovie.nhn'
html_text =requests.get(url).text;
soup_obj =BeautifulSoup(html_text, 'html.parser')
keys =soup_obj.select('div.tit3')
~~~

위 코드에서 마지막줄에 BeautifulSoup의 select를 이용했습니다.   

여기서 select안에 있는게 의미하는 것은 html 코드 안에서 div 태그 안에 있는 class가 tit3인 정보를 keys에 가져온겂니다.   

(select를 이용하는 방법은 데이터공방님의 블로그를 참조했습니다. 주소 : <https://m.blog.naver.com/kiddwannabe/221177292446>)   

print( type(keys) )을 출력해보면 "<class 'bs4.element.ResultSet'>"가 나옵니다.   

div 태그안에 있는 class가 tit3인 정보를 모아놓은 것이겠죠? 근데 keys안에 있는 내용을 출력해보면 리스트의 형태로 되어있습니다.   

div 태그안에 있는 clas가 tit3인 정보가 여러개인 것입니다.   

print( type(keys[0]) )을 출력해보면 "<class 'bs4.element.Tag'>"가 나옵니다.   

그 중 keys[0]과 keys[0].text를 출력해보겠습니다.   

~~~python
print('keys[0]: ', keys[0])
print('kes[0].text: ', keys[0].text)
~~~

결과

~~~text
keys[0]:  <div class="tit3">
<a href="/movie/bi/mi/basic.nhn?code=188909" title="강철비2: 정상회담">강철비2: 정상회담</a>
</div>
kes[0].text:  
강철비2: 정상회담
~~~

결과가 위와 같이 나왔습니다.   

keys[0]은 select를 이용해서 얻은 값 중 한개이고, keys[0]은 그것의 텍스트를 출력했습니다.   

위에서 보면 keys[0].text의 출력 문자열의 앞 뒤에 '\n'가 붙어있습니다.   

그것은 "keys[0].text.replace('\n', '')" replace를 이용하면 없앨 수 있습니다.      

## 간단한 예제

위에서 페이지의 정보를 가져와봤습니다.   

이번에는 위에서 했던것과 비슷하게 네이버 영화 랭킹 페이지에서 영화 순위를 출력해보는 예제를 다뤄보겠습니다.   

~~~python
import requests
from bs4 import BeautifulSoup

url ='https://movie.naver.com/movie/sdb/rank/rmovie.nhn'
html_text =requests.get(url).text;
soup_obj =BeautifulSoup(html_text, 'html.parser')
keys =soup_obj.select('div.tit3')

movie_ranking =[]
for key in keys:
    text =key.text
    movie_ranking.append(text.replace('\n', ''))

ranking =1
for movie in movie_ranking:
    print('순위: {0}위 \t 영화 이름: {1}'.format(ranking, movie))
    ranking +=1
~~~

결과   

![2020_08_01_1](../images/2020_08_01_1.PNG)